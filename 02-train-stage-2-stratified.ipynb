{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71634679-eee9-4810-8250-0f27e067735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -y -q pytorch=2.1.2 torchvision pytorch-cuda=12.1 scikit-learn h5py -c pytorch -c nvidia -c anaconda -c conda-forge\n",
    "!pip install -q lightning torchmetrics pyts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1945390-0f45-497c-b4d5-c8410c5378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a98a0-03be-40f8-9835-b7f12d82b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from pyts.image import RecurrencePlot\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models import vgg11\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "import pickle\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Optimization for GPU with Tensor cores\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS = 20\n",
    "\n",
    "class ActivityDataset(Dataset):\n",
    "    def __init__(self, include_sparse_data=False, transform=None):\n",
    "        self.fp = h5py.File('./ukdale-transformed-data.h5', swmr=True)\n",
    "        self.activity = self.fp['activity'][:]\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        if not include_sparse_data:\n",
    "            self.indices = np.argwhere(self.fp['activity'][:])\n",
    "        else:\n",
    "            self.indices = np.arange(len(self.activity), dtype=int)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        idx = self.indices[index]\n",
    "        \n",
    "        sample = self.fp['X'][idx]\n",
    "        sample = np.squeeze(sample)\n",
    "        sample = np.expand_dims(sample, axis=-1)\n",
    "        \n",
    "        label = self.fp['y'][idx]\n",
    "        label = np.squeeze(label)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, np.int64(label)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.fp.close()\n",
    "\n",
    "ActivityDataset(include_sparse_data=True)[0];\n",
    "assert len(ActivityDataset(include_sparse_data=True)) > len(ActivityDataset(include_sparse_data=False))\n",
    "print(len(ActivityDataset(include_sparse_data=True)), len(ActivityDataset(include_sparse_data=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f2f94-5c2c-4f13-999d-ed02a03048e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels in the dataset are numerical. Let's create mapper to meaningful names.\n",
    "mapper = {\n",
    "    value: idx\n",
    "    for idx, value in\n",
    "    enumerate(['HEKA', 'HTPC', 'boiler', 'computer monitor', 'desktop computer', 'fridge/freezer', 'laptop computer', 'light', 'microwave', 'server computer', 'television', 'washer dryer'])\n",
    "}\n",
    "reverse_mapper = { v:k for k, v in mapper.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90257c-9e55-4701-9d62-82d967d6c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(mapper); num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36359cb5-d35a-4241-a4fb-48ca88122b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert1to3channels:\n",
    "    def __call__(self, x):\n",
    "        return np.concatenate((x, x, x), axis=-1)\n",
    "    \n",
    "transform = T.Compose([\n",
    "    #RecurrencePlotTransform(), # time-series to recurrence plot transformation\n",
    "    Convert1to3channels(), # To fulfill CNN requirement of having 3 channels\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "class Net(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = vgg11()  # VGG11 (without BN) as a backbone\n",
    "        \n",
    "        # Loss metric\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Evaluation metrics to see progress\n",
    "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #x = torch.abs(x[:, :, :, None] - x[:, :, None, :]) # TODO: recurrencePlot on GPU?\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        inputs, _ = batch\n",
    "        logits = self(inputs)\n",
    "        return logits\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        logits = self(inputs)\n",
    "\n",
    "        loss = self.criterion(logits, targets)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        logits = self(inputs)\n",
    "        loss = self.criterion(logits, targets)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        # validation metrics\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(predictions, targets)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc50d5-9f8a-4a4c-baae-819b5250f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "dataloader_kwargs = dict(\n",
    "    batch_size=128,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    #prefetch_factor=16,\n",
    "    pin_memory_device='cuda',\n",
    ")\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "#kfold = model_selection.StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "CHECKPOINT_DIR = Path('./checkpoints/').resolve()\n",
    "assert CHECKPOINT_DIR.exists()\n",
    "\n",
    "\n",
    "results = {}\n",
    "results[\"mapper\"] = mapper\n",
    "results[\"with-sparse\"] = []\n",
    "results[\"without-sparse\"] = []\n",
    "\n",
    "\n",
    "for USE_SPARSE in [True, False]: # switch between sparse and non-sparse dataset\n",
    "    dataset = ActivityDataset(\n",
    "        include_sparse_data=USE_SPARSE,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for model_id, (train_ids, val_ids) in enumerate(kfold.split(dataset), start=1):\n",
    "        checkpoint_path = CHECKPOINT_DIR / (f'model-{model_id+1:02}-sparse.ckpt' if USE_SPARSE else f'model-{model_id+1:02}.ckpt')\n",
    "        \n",
    "        if checkpoint_path.exists():\n",
    "            continue\n",
    "            \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        L.seed_everything(SEED + model_id)\n",
    "        \n",
    "        model = Net()\n",
    "        \n",
    "        train_loader = DataLoader(dataset, sampler=SubsetRandomSampler(train_ids), **dataloader_kwargs)\n",
    "        val_loader = DataLoader(dataset, sampler=SubsetRandomSampler(val_ids), **dataloader_kwargs)\n",
    "        \n",
    "        trainer = L.Trainer(\n",
    "            benchmark=True, # Enable GPU specific optimizations\n",
    "            max_epochs=100,\n",
    "            accelerator='gpu',\n",
    "            devices=1,\n",
    "            precision='bf16-mixed', # Mixed precision to speedup the process\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0, patience=3, mode='min', verbose=True)],\n",
    "            #fast_dev_run=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            trainer.fit(model, train_loader, val_loader)\n",
    "            trainer.save_checkpoint(checkpoint_path)\n",
    "            print(f\"{model_id+1} complete!\")\n",
    "        except RuntimeError: # in rare cases might fail\n",
    "            pass\n",
    "\n",
    "        outputs = trainer.predict(model, val_loader)\n",
    "        #y_pred = torch.cat(y_pred).cpu().numpy()\n",
    "        #break\n",
    "        y_true = torch.cat([x[0] for x in outputs]).cpu().numpy()\n",
    "        y_pred = torch.cat([x[1] for x in outputs]).cpu().numpy()\n",
    "        y_proba = torch.cat([x[2] for x in outputs]).cpu().float().numpy()\n",
    "\n",
    "        key = \"with-sparse\" if USE_SPARSE else \"without-sparse\"\n",
    "        results[key].append(dict(y_true=y_true, y_pred=y_pred, y_proba=y_proba))\n",
    "        \n",
    "        del trainer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470fae1-e871-4274-901b-7876d026d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(results, 'results.joblib', compress=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
